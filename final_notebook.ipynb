{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, Input, Embedding\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import yfinance as yf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oneHotEncode(df, columns):\n",
    "    all_years = [2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024]\n",
    "    all_months = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
    "    all_days = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]\n",
    "    all_days_of_week = [1, 2, 3, 4, 5, 6, 7]\n",
    "\n",
    "    one_hot_encoded_df = pd.get_dummies(df, columns = columns)\n",
    "    one_hot_encoded_df = one_hot_encoded_df.reindex(columns=[f'year_{year}' for year in all_years], fill_value=0)\n",
    "    one_hot_encoded_df = one_hot_encoded_df.reindex(columns=[f'month_{month}' for month in all_months], fill_value=0)\n",
    "    one_hot_encoded_df = one_hot_encoded_df.reindex(columns=[f'day_{day}' for day in all_days], fill_value=0)\n",
    "    one_hot_encoded_df = one_hot_encoded_df.reindex(columns=[f'day_of_week_{day_of_week}' for day_of_week in all_days_of_week], fill_value=0)\n",
    "\n",
    "    print(one_hot_encoded_df.head())\n",
    "    print(df.head())\n",
    "\n",
    "    new_dataframe = pd.concat([df, one_hot_encoded_df], axis=1)\n",
    "\n",
    "    return new_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of all stocks\n",
    "stocks = ['aapl', 'amzn', 'googl', 'msft', 'tsla']\n",
    "\n",
    "# Get rid of annoying errors that don't matter\n",
    "pd.set_option('mode.chained_assignment', None)\n",
    "\n",
    "# Create the scaler\n",
    "sentimentScaler = MinMaxScaler()\n",
    "stockScaler = MinMaxScaler()\n",
    "\n",
    "# Create the datasets from the csvs\n",
    "# Could use a lot of work to reduce loc but it doesn't matter for marks\n",
    "def dataFrames(stock):\n",
    "\n",
    "    # Create dataframes for the tweets and prices \n",
    "    tweet_df = pd.read_csv('csv/' + stock + '.csv')\n",
    "    tweet_1_df = pd.read_csv('csv/' + stock + '_1.csv')\n",
    "    price_df = pd.read_csv('csv/' + stock + '_price.csv')\n",
    "\n",
    "    # Rename the date column so they match between dataframes\n",
    "    tweet_df.rename(columns = {'DATE': 'date'}, inplace = True)\n",
    "    tweet_1_df.rename(columns = {'Date': 'date'}, inplace = True)\n",
    "    price_df.rename(columns = {'Date' : 'date', 'Close/Last': 'stockPrice'}, inplace = True)\n",
    "\n",
    "    # Rename the score column so they match between dataframes\n",
    "    tweet_df.rename(columns = {'TEXTBLOB_POLARITY': 'sentimentScore'}, inplace = True)\n",
    "    tweet_1_df.rename(columns = {'score': 'sentimentScore'}, inplace = True)\n",
    "\n",
    "    # Remove the $ sign in stock price, and convert columns to float64 (may be redundant)\n",
    "    price_df['stockPrice'] = price_df['stockPrice'].str.replace('$', '')\n",
    "    tweet_df['sentimentScore'] = pd.to_numeric(tweet_df['sentimentScore'])\n",
    "    tweet_1_df['sentimentScore'] = pd.to_numeric(tweet_1_df['sentimentScore'])\n",
    "    price_df['stockPrice'] = pd.to_numeric(price_df['stockPrice'])\n",
    "\n",
    "    # Convert the dates to pd datetime\n",
    "    price_df['date'] = pd.to_datetime(price_df['date'])\n",
    "    tweet_df['date'] = pd.to_datetime(tweet_df['date'], dayfirst = True)\n",
    "    tweet_1_df['date'] = pd.to_datetime(tweet_1_df['date'])\n",
    "\n",
    "    # Reduce to only nessicary columns\n",
    "    tweet_df = tweet_df[['date', 'sentimentScore']]\n",
    "    tweet_1_df = tweet_1_df[['date', 'sentimentScore']]\n",
    "    price_df = price_df[['date', 'stockPrice']]\n",
    "\n",
    "    # Merge the dataframes based on the date column\n",
    "    merged_1_df = pd.merge(tweet_df, price_df, on = 'date')\n",
    "    merged_2_df = pd.merge(tweet_1_df, price_df, on = 'date')\n",
    "    merged_df = pd.merge(merged_1_df, merged_2_df, how = 'outer')\n",
    "\n",
    "    # Drop all duplicates\n",
    "    final_df = merged_df.drop_duplicates()\n",
    "\n",
    "    # Convert date to specific variables\n",
    "    final_df['year'] = final_df['date'].dt.year\n",
    "    final_df['month'] = final_df['date'].dt.month\n",
    "    final_df['day'] = final_df['date'].dt.day\n",
    "    final_df['day_of_week'] = final_df['date'].dt.dayofweek\n",
    "\n",
    "    # One-hot encoding, should be done for all for financial data\n",
    "    final_df = oneHotEncode(final_df, ['year', 'month', 'day', 'day_of_week'])\n",
    "    final_df = pd.get_dummies(final_df, columns = ['year', 'month', 'day', 'day_of_week'])\n",
    "\n",
    "    # Drop the original date column\n",
    "    final_df = final_df.drop(columns = ['date'])\n",
    "\n",
    "    # Save to csv for double checking\n",
    "    #final_df.to_csv('final_' + stock + '.csv', index = False)\n",
    "\n",
    "    # Scale the numerical values\n",
    "    final_df[['sentimentScore']] = sentimentScaler.fit_transform(final_df[['sentimentScore']])\n",
    "    final_df[['stockPrice']] = stockScaler.fit_transform(final_df[['stockPrice']])\n",
    "\n",
    "    # Split into X and y\n",
    "    X = final_df.drop(columns = ['stockPrice'])\n",
    "    y = final_df['stockPrice']\n",
    "\n",
    "    # Split the datasets into training and test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
    "\n",
    "    '''# Reshape the datasets\n",
    "    X_train_reshaped = reshape_data(X_train)\n",
    "    X_test_reshaped = reshape_data(X_test)'''\n",
    "\n",
    "    # NOTE: Sequence length (1) could be improved by making one of the commented functions above work.\n",
    "    X_train_reshaped = np.reshape(X_train, (-1, 1, X_train.shape[1]))\n",
    "    X_test_reshaped = np.reshape(X_test, (-1,  1, X_test.shape[1]))\n",
    "    y_train_array = y_train.values\n",
    "    y_test_array = y_test.values\n",
    "\n",
    "    # Return the training and test datasets\n",
    "    return X_train_reshaped, X_test_reshaped, y_train_array, y_test_array, X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    day_of_week_1  day_of_week_2  day_of_week_3  day_of_week_4  day_of_week_5  \\\n",
      "0               0              0              0              0              0   \n",
      "3               0              0              0              0              0   \n",
      "4               0              0              0              0              0   \n",
      "10              0              0              0              0              0   \n",
      "18              0              0              0              0              0   \n",
      "\n",
      "    day_of_week_6  day_of_week_7  \n",
      "0               0              0  \n",
      "3               0              0  \n",
      "4               0              0  \n",
      "10              0              0  \n",
      "18              0              0  \n",
      "         date  sentimentScore  stockPrice  year  month  day  day_of_week\n",
      "0  2015-01-02            -0.8     27.3325  2015      1    2            4\n",
      "3  2015-01-02            -0.7     27.3325  2015      1    2            4\n",
      "4  2015-01-02            -0.6     27.3325  2015      1    2            4\n",
      "10 2015-01-02            -0.5     27.3325  2015      1    2            4\n",
      "18 2015-01-02            -0.4     27.3325  2015      1    2            4\n",
      "    day_of_week_1  day_of_week_2  day_of_week_3  day_of_week_4  day_of_week_5  \\\n",
      "0               0              0              0              0              0   \n",
      "1               0              0              0              0              0   \n",
      "9               0              0              0              0              0   \n",
      "10              0              0              0              0              0   \n",
      "12              0              0              0              0              0   \n",
      "\n",
      "    day_of_week_6  day_of_week_7  \n",
      "0               0              0  \n",
      "1               0              0  \n",
      "9               0              0  \n",
      "10              0              0  \n",
      "12              0              0  \n",
      "         date  sentimentScore  stockPrice  year  month  day  day_of_week\n",
      "0  2015-01-02            -0.9      15.426  2015      1    2            4\n",
      "1  2015-01-02            -0.6      15.426  2015      1    2            4\n",
      "9  2015-01-02            -0.5      15.426  2015      1    2            4\n",
      "10 2015-01-02            -0.4      15.426  2015      1    2            4\n",
      "12 2015-01-02            -0.3      15.426  2015      1    2            4\n",
      "   day_of_week_1  day_of_week_2  day_of_week_3  day_of_week_4  day_of_week_5  \\\n",
      "0              0              0              0              0              0   \n",
      "1              0              0              0              0              0   \n",
      "4              0              0              0              0              0   \n",
      "6              0              0              0              0              0   \n",
      "9              0              0              0              0              0   \n",
      "\n",
      "   day_of_week_6  day_of_week_7  \n",
      "0              0              0  \n",
      "1              0              0  \n",
      "4              0              0  \n",
      "6              0              0  \n",
      "9              0              0  \n",
      "        date  sentimentScore  stockPrice  year  month  day  day_of_week\n",
      "0 2015-01-02            -0.4     26.4775  2015      1    2            4\n",
      "1 2015-01-02            -0.3     26.4775  2015      1    2            4\n",
      "4 2015-01-02            -0.2     26.4775  2015      1    2            4\n",
      "6 2015-01-02            -0.1     26.4775  2015      1    2            4\n",
      "9 2015-01-02             0.0     26.4775  2015      1    2            4\n",
      "    day_of_week_1  day_of_week_2  day_of_week_3  day_of_week_4  day_of_week_5  \\\n",
      "0               0              0              0              0              0   \n",
      "3               0              0              0              0              0   \n",
      "5               0              0              0              0              0   \n",
      "13              0              0              0              0              0   \n",
      "22              0              0              0              0              0   \n",
      "\n",
      "    day_of_week_6  day_of_week_7  \n",
      "0               0              0  \n",
      "3               0              0  \n",
      "5               0              0  \n",
      "13              0              0  \n",
      "22              0              0  \n",
      "         date  sentimentScore  stockPrice  year  month  day  day_of_week\n",
      "0  2015-01-02            -0.4       46.76  2015      1    2            4\n",
      "3  2015-01-02            -0.3       46.76  2015      1    2            4\n",
      "5  2015-01-02            -0.2       46.76  2015      1    2            4\n",
      "13 2015-01-02            -0.1       46.76  2015      1    2            4\n",
      "22 2015-01-02             0.0       46.76  2015      1    2            4\n",
      "   day_of_week_1  day_of_week_2  day_of_week_3  day_of_week_4  day_of_week_5  \\\n",
      "0              0              0              0              0              0   \n",
      "1              0              0              0              0              0   \n",
      "3              0              0              0              0              0   \n",
      "4              0              0              0              0              0   \n",
      "5              0              0              0              0              0   \n",
      "\n",
      "   day_of_week_6  day_of_week_7  \n",
      "0              0              0  \n",
      "1              0              0  \n",
      "3              0              0  \n",
      "4              0              0  \n",
      "5              0              0  \n",
      "        date  sentimentScore  stockPrice  year  month  day  day_of_week\n",
      "0 2015-01-02            -0.8     14.6207  2015      1    2            4\n",
      "1 2015-01-02            -0.6     14.6207  2015      1    2            4\n",
      "3 2015-01-02            -0.5     14.6207  2015      1    2            4\n",
      "4 2015-01-02            -0.4     14.6207  2015      1    2            4\n",
      "5 2015-01-02            -0.3     14.6207  2015      1    2            4\n"
     ]
    }
   ],
   "source": [
    "all_data = {}  # Dictionary to hold data for all stocks\n",
    "\n",
    "for stock in stocks:\n",
    "    # Run the 'dataFrames' function for each stock\n",
    "    data = dataFrames(stock)\n",
    "    \n",
    "    # Store the datasets in the dictionary\n",
    "    all_data[stock] = {\n",
    "        'X_train': data[0],\n",
    "        'X_test': data[1],\n",
    "        'y_train': data[2],\n",
    "        'y_test': data[3],\n",
    "        'X': data[4]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "674/674 [==============================] - 7s 6ms/step - loss: 0.0099\n",
      "Epoch 2/20\n",
      "674/674 [==============================] - 4s 6ms/step - loss: 0.0022\n",
      "Epoch 3/20\n",
      "674/674 [==============================] - 4s 6ms/step - loss: 0.0016\n",
      "Epoch 4/20\n",
      "674/674 [==============================] - 5s 7ms/step - loss: 0.0013\n",
      "Epoch 5/20\n",
      "674/674 [==============================] - 5s 7ms/step - loss: 0.0012\n",
      "Epoch 6/20\n",
      "674/674 [==============================] - 5s 7ms/step - loss: 0.0010\n",
      "Epoch 7/20\n",
      "674/674 [==============================] - 5s 7ms/step - loss: 9.5053e-04\n",
      "Epoch 8/20\n",
      "674/674 [==============================] - 4s 7ms/step - loss: 8.3185e-04\n",
      "Epoch 9/20\n",
      "674/674 [==============================] - 4s 6ms/step - loss: 7.3390e-04\n",
      "Epoch 10/20\n",
      "674/674 [==============================] - 5s 7ms/step - loss: 7.1445e-04\n",
      "Epoch 11/20\n",
      "674/674 [==============================] - 5s 7ms/step - loss: 6.3934e-04\n",
      "Epoch 12/20\n",
      "674/674 [==============================] - 5s 7ms/step - loss: 5.8638e-04\n",
      "Epoch 13/20\n",
      "674/674 [==============================] - 5s 7ms/step - loss: 5.4098e-04\n",
      "Epoch 14/20\n",
      "674/674 [==============================] - 5s 8ms/step - loss: 5.4797e-04\n",
      "Epoch 15/20\n",
      "674/674 [==============================] - 4s 6ms/step - loss: 5.0092e-04\n",
      "Epoch 16/20\n",
      "674/674 [==============================] - 4s 6ms/step - loss: 4.7683e-04\n",
      "Epoch 17/20\n",
      "674/674 [==============================] - 4s 7ms/step - loss: 4.4600e-04\n",
      "Epoch 18/20\n",
      "674/674 [==============================] - 5s 7ms/step - loss: 4.3911e-04\n",
      "Epoch 19/20\n",
      "674/674 [==============================] - 5s 7ms/step - loss: 4.1936e-04\n",
      "Epoch 20/20\n",
      "674/674 [==============================] - 4s 7ms/step - loss: 3.9726e-04\n",
      "169/169 [==============================] - 1s 3ms/step - loss: 9.1777e-05\n",
      "aapl Test Loss:  9.177739411825314e-05\n"
     ]
    }
   ],
   "source": [
    "# Initialize dictionary to store the models for each stock\n",
    "models = {}\n",
    "\n",
    "for stock, datasets in all_data.items():\n",
    "    X_train = datasets['X_train']\n",
    "    y_train = datasets['y_train']\n",
    "    X_test = datasets['X_test']\n",
    "    y_test = datasets['y_test']\n",
    "    \n",
    "    # Define the model for current stock\n",
    "    model = Sequential([\n",
    "        Input(shape=X_train.shape[1:]),\n",
    "        LSTM(units=100, return_sequences=True, input_shape=(X_train.shape[1:], 1)),\n",
    "        Dropout(0.2), \n",
    "        LSTM(units=100, return_sequences=True),\n",
    "        Dropout(0.2),\n",
    "        LSTM(units=100),\n",
    "        Dropout(0.2),\n",
    "        Dense(units=1)\n",
    "    ])\n",
    "\n",
    "    # Convert data type of input arrays to float32\n",
    "    X_train = X_train.astype('float32')\n",
    "    y_train = y_train.astype('float32')\n",
    "    X_test = X_test.astype('float32')\n",
    "    y_test = y_test.astype('float32')\n",
    "\n",
    "    # Compile model using the Adam optimizer\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "\n",
    "    # Ensure that X_train and y_train have the same number of samples (if necessary)\n",
    "    X_train = X_train[:len(y_train)]\n",
    "\n",
    "    # Fit the model with verbose settings for debugging\n",
    "    model.fit(X_train, y_train, epochs=20, batch_size=32, verbose=1)\n",
    "\n",
    "    # Evaluate the model and print the loss\n",
    "    loss = model.evaluate(X_test, y_test)\n",
    "    print(f\"{stock} Test Loss: \", loss)\n",
    "\n",
    "    # Store the model in the dictionary\n",
    "    models[stock] = model\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aapl Random Forest Test Loss:  4.1968745959621414e-10\n",
      "aapl Linear Regression Test Loss:  0.011016191169306345\n",
      "aapl Decision Tree Test Loss:  1.9007839717704197e-32\n",
      "amzn Random Forest Test Loss:  1.1315437327411813e-09\n",
      "amzn Linear Regression Test Loss:  0.010219437240305293\n",
      "amzn Decision Tree Test Loss:  3.912598012761351e-32\n",
      "googl Random Forest Test Loss:  5.083253809348119e-09\n",
      "googl Linear Regression Test Loss:  0.006389820932366614\n",
      "googl Decision Tree Test Loss:  3.846647740672896e-32\n",
      "msft Random Forest Test Loss:  1.955834244360713e-09\n",
      "msft Linear Regression Test Loss:  0.00498345298212116\n",
      "msft Decision Tree Test Loss:  7.520586315893814e-33\n",
      "tsla Random Forest Test Loss:  8.65813642465698e-09\n",
      "tsla Linear Regression Test Loss:  0.02937480932458805\n",
      "tsla Decision Tree Test Loss:  1.2126277193947334e-32\n"
     ]
    }
   ],
   "source": [
    "for stock, datasets in all_data.items():\n",
    "    X_train = datasets['X_train']\n",
    "    y_train = datasets['y_train']\n",
    "    X_test = datasets['X_test']\n",
    "    y_test = datasets['y_test']\n",
    "\n",
    "    # Flatten X data for compatibility with RandomForest, Linear Regression, and Decision Tree\n",
    "    num_samples, timesteps, num_features = X_train.shape\n",
    "    X_train_2d = X_train.reshape(num_samples, timesteps * num_features)\n",
    "    X_test_2d = X_test.reshape(X_test.shape[0], timesteps * num_features)\n",
    "\n",
    "    # Store processed data\n",
    "    all_data[stock] = (X_train_2d, X_test_2d, y_train, y_test)\n",
    "\n",
    "# Initialize dictionaries for models and their performances\n",
    "rf_models = {}\n",
    "rf_losses = {}\n",
    "lr_models = {}\n",
    "lr_losses = {}\n",
    "dt_models = {}\n",
    "dt_losses = {}\n",
    "\n",
    "for stock, (X_train_2d, X_test_2d, y_train, y_test) in all_data.items():\n",
    "    # RandomForestRegressor\n",
    "    rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    rf_model.fit(X_train_2d, y_train)\n",
    "    rf_predictions = rf_model.predict(X_test_2d)\n",
    "    rf_loss = mean_squared_error(y_test, rf_predictions)\n",
    "    rf_models[stock] = rf_model\n",
    "    rf_losses[stock] = rf_loss\n",
    "    print(f\"{stock} Random Forest Test Loss: \", rf_loss)\n",
    "\n",
    "    # Linear Regression\n",
    "    lr_model = LinearRegression()\n",
    "    lr_model.fit(X_train_2d, y_train)\n",
    "    lr_predictions = lr_model.predict(X_test_2d)\n",
    "    lr_loss = mean_squared_error(y_test, lr_predictions)\n",
    "    lr_models[stock] = lr_model\n",
    "    lr_losses[stock] = lr_loss\n",
    "    print(f\"{stock} Linear Regression Test Loss: \", lr_loss)\n",
    "    \n",
    "    # Decision Tree Regressor\n",
    "    dt_model = DecisionTreeRegressor(random_state=42)\n",
    "    dt_model.fit(X_train_2d, y_train)\n",
    "    dt_predictions = dt_model.predict(X_test_2d)\n",
    "    dt_loss = mean_squared_error(y_test, dt_predictions)\n",
    "    dt_models[stock] = dt_model\n",
    "    dt_losses[stock] = dt_loss\n",
    "    print(f\"{stock} Decision Tree Test Loss: \", dt_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aapl Gaussian Naive Bayes Test Accuracy:  0.9658373561084292\n",
      "amzn Gaussian Naive Bayes Test Accuracy:  0.9983983983983984\n",
      "googl Gaussian Naive Bayes Test Accuracy:  0.9938168846611177\n",
      "msft Gaussian Naive Bayes Test Accuracy:  0.9352248394004282\n",
      "tsla Gaussian Naive Bayes Test Accuracy:  0.9822022296107961\n"
     ]
    }
   ],
   "source": [
    "# Initialize dictionaries for GaussianNB models and their accuracies\n",
    "gaussian_nb_models = {}\n",
    "gaussian_nb_accuracies = {}\n",
    "\n",
    "# Label encoder to convert continuous labels to discrete integers\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "for stock, (X_train_2d, X_test_2d, y_train, y_test) in all_data.items():\n",
    "    # Convert continuous labels to discrete integers\n",
    "    y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "    y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "    # Define and initialize the Gaussian Naive Bayes model\n",
    "    bayes_model = GaussianNB()\n",
    "\n",
    "    # Fit the model with the reshaped 2D training data and encoded labels\n",
    "    bayes_model.fit(X_train_2d, y_train_encoded)\n",
    "\n",
    "    # Make predictions on the reshaped 2D test data\n",
    "    bayes_predictions = bayes_model.predict(X_test_2d)\n",
    "\n",
    "    # Calculate and print the accuracy for the test set predictions\n",
    "    accuracy = accuracy_score(y_test_encoded, bayes_predictions)\n",
    "    print(f\"{stock} Gaussian Naive Bayes Test Accuracy: \", accuracy)\n",
    "\n",
    "    # Store the model and its accuracy\n",
    "    gaussian_nb_models[stock] = bayes_model\n",
    "    gaussian_nb_accuracies[stock] = accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for aapl:\n",
      "169/169 [==============================] - 0s 2ms/step\n",
      "[[0.08895484]\n",
      " [0.39178103]\n",
      " [0.08344239]\n",
      " ...\n",
      " [0.14679654]\n",
      " [0.4975949 ]\n",
      " [0.03982714]]\n",
      "Predictions for amzn:\n",
      "157/157 [==============================] - 0s 3ms/step\n",
      "[[0.1154601 ]\n",
      " [0.3708629 ]\n",
      " [0.49733722]\n",
      " ...\n",
      " [0.42259875]\n",
      " [0.50593984]\n",
      " [0.39796114]]\n",
      "Predictions for googl:\n",
      "132/132 [==============================] - 0s 3ms/step\n",
      "[[0.5665148 ]\n",
      " [0.29367352]\n",
      " [0.03488001]\n",
      " ...\n",
      " [0.35470587]\n",
      " [0.4304074 ]\n",
      " [0.33446342]]\n",
      "Predictions for msft:\n",
      "117/117 [==============================] - 0s 3ms/step\n",
      "[[0.02951252]\n",
      " [0.65561527]\n",
      " [0.24362886]\n",
      " ...\n",
      " [0.6349383 ]\n",
      " [0.34257722]\n",
      " [0.10590923]]\n",
      "Predictions for tsla:\n",
      "160/160 [==============================] - 0s 3ms/step\n",
      "[[0.44474643]\n",
      " [0.55493176]\n",
      " [0.12150039]\n",
      " ...\n",
      " [0.12450145]\n",
      " [0.57663286]\n",
      " [0.07778883]]\n"
     ]
    }
   ],
   "source": [
    "predictions = {}\n",
    "true_price = {}\n",
    "final_tweets = {}\n",
    "symbols = ['AAPL', 'AMZN', 'GOOGL', 'MSFT', 'TSLA']\n",
    "\n",
    "def predictData(stock):\n",
    "\n",
    "    # Get the model for the stock\n",
    "    current_price_data = [[168.45], [168.45], [168.45], [168.45], [168.45], [168.45], [168.45], [168.45], [168.45], [168.45]]\n",
    "    current_price_data = pd.DataFrame(current_price_data, columns=['stockPrice'])\n",
    "    current_price_data['stockPrice'] = pd.to_numeric(current_price_data['stockPrice'])\n",
    "    current_price_data['stockPrice'] = stockScaler.transform(current_price_data[['stockPrice']])\n",
    "    \n",
    "    # Get the true price\n",
    "    y_true = current_price_data.values\n",
    "    y_true = stockScaler.inverse_transform(y_true)\n",
    "\n",
    "    # Get the current tweets\n",
    "    current_tweets = pd.read_csv('csv/output.csv')\n",
    "    current_tweets.rename(columns = {'Date': 'date'}, inplace = True)\n",
    "    current_tweets.rename(columns = {'Sentiment': 'sentimentScore'}, inplace = True)\n",
    "    \n",
    "    # Remove the $ sign in stock price, and convert columns to float64 (may be redundant)\n",
    "    sentiment_mapping = {'Negative': -1, 'Neutral': 0, 'Positive': 1}\n",
    "    current_tweets['sentimentScore'] = current_tweets['sentimentScore'].map(sentiment_mapping)\n",
    "    current_tweets = current_tweets[['date', 'sentimentScore']]\n",
    "    current_tweets['sentimentScore'] = pd.to_numeric(current_tweets['sentimentScore'])\n",
    "    current_tweets['sentimentScore'] = sentimentScaler.transform(current_tweets[['sentimentScore']])\n",
    "\n",
    "    # Convert the dates to pd datetime\n",
    "    current_tweets['date'] = pd.to_datetime(current_tweets['date'])\n",
    "    current_tweets['year'] = current_tweets['date'].dt.year\n",
    "    current_tweets['month'] = current_tweets['date'].dt.month\n",
    "    current_tweets['day'] = current_tweets['date'].dt.day\n",
    "    current_tweets['day_of_week'] = current_tweets['date'].dt.dayofweek\n",
    "\n",
    "    '''# Final changes\n",
    "    X = current_tweets.drop(columns = ['date'])\n",
    "    X = np.reshape(X, (-1, 1, X.shape[1]))\n",
    "    X = X.astype('float32')'''\n",
    "\n",
    "    # Use old data\n",
    "    X = all_data[stock]['X_test']\n",
    "    X = X.astype('float32')\n",
    "\n",
    "    y_true = all_data[stock]['y_test']\n",
    "\n",
    "    y_test = model.predict(X)\n",
    "    print(y_test)\n",
    "    y_test = stockScaler.inverse_transform(y_test)\n",
    "    \n",
    "    y_true = y_true.reshape(-1, 1)\n",
    "    y_true = stockScaler.inverse_transform(y_true)\n",
    "    predictions[stock] = y_test\n",
    "    true_price[stock] = y_true\n",
    "    final_tweets[stock] = X\n",
    "\n",
    "# Predict for each stock\n",
    "for stock in stocks:\n",
    "    print(f\"Predictions for {stock}:\")\n",
    "    predictData(stock)\n",
    "    \n",
    "    final_tweets[stock] = final_tweets[stock].reshape(final_tweets[stock].shape[0], final_tweets[stock].shape[2])\n",
    "\n",
    "    X_df = pd.DataFrame(final_tweets[stock], columns = [all_data[stock]['X'].columns])\n",
    "\n",
    "    y_pred_merged = pd.DataFrame(predictions[stock], columns = ['stockPrice'])\n",
    "    y_true_merged = pd.DataFrame(true_price[stock], columns = ['stockPrice'])\n",
    "\n",
    "    y_pred_merged.to_csv('final_' + stock + '_predicitions_indexed.csv', index = False)\n",
    "    y_true_merged.to_csv('final_' + stock + '_true_price_indexed.csv', index = False)\n",
    "    X_df.to_csv('final_' + stock + '_tweets_indexed.csv', index = False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[133], line 33\u001b[0m\n\u001b[0;32m     30\u001b[0m     plt\u001b[38;5;241m.\u001b[39mtight_layout()\n\u001b[0;32m     31\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m---> 33\u001b[0m \u001b[43mplotPredictions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrue_price\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43maapl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43maapl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal_tweets\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43maapl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[133], line 6\u001b[0m, in \u001b[0;36mplotPredictions\u001b[1;34m(y_true, y_pred, X)\u001b[0m\n\u001b[0;32m      4\u001b[0m X_df \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m      5\u001b[0m X_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(X_df, columns \u001b[38;5;241m=\u001b[39m [all_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maapl\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mcolumns])\n\u001b[1;32m----> 6\u001b[0m dates \u001b[38;5;241m=\u001b[39m [X_df(year\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2022\u001b[39m, month\u001b[38;5;241m=\u001b[39mmonth\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m, day\u001b[38;5;241m=\u001b[39mday\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m year, month, day \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(np\u001b[38;5;241m.\u001b[39margmax(X, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))]\n\u001b[0;32m      8\u001b[0m X_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m X_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myear\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m X_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmonth\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m X_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mday\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\n\u001b[0;32m      9\u001b[0m X_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(X_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Cell \u001b[1;32mIn[133], line 6\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      4\u001b[0m X_df \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m      5\u001b[0m X_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(X_df, columns \u001b[38;5;241m=\u001b[39m [all_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maapl\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mcolumns])\n\u001b[1;32m----> 6\u001b[0m dates \u001b[38;5;241m=\u001b[39m [X_df(year\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2022\u001b[39m, month\u001b[38;5;241m=\u001b[39mmonth\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m, day\u001b[38;5;241m=\u001b[39mday\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m year, month, day \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(np\u001b[38;5;241m.\u001b[39margmax(X, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))]\n\u001b[0;32m      8\u001b[0m X_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m X_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myear\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m X_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmonth\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m X_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mday\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\n\u001b[0;32m      9\u001b[0m X_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(X_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)"
     ]
    }
   ],
   "source": [
    "def plotPredictions(y_true, y_pred, X): \n",
    "\n",
    "\n",
    "    X_df = X.reshape(-1, X.shape[-1])\n",
    "    X_df = pd.DataFrame(X_df, columns = [all_data['aapl']['X'].columns])\n",
    "\n",
    "    X_df['date'] = X_df['year'].astype(str) + '-' + X_df['month'].astype(str) + '-' + X_df['day'].astype(str)\n",
    "    X_df['date'] = pd.to_datetime(X_df['date'])\n",
    "\n",
    "    X_df.drop(columns = ['year', 'month', 'day', 'day_of_week', 'sentimentScore'], inplace = True)\n",
    "\n",
    "    y_pred_merged = pd.DataFrame(y_pred, columns = ['stockPrice'])\n",
    "    y_true_merged = pd.DataFrame(y_true, columns = ['stockPrice'])\n",
    "\n",
    "    y_pred_merged = pd.merge(X_df, y_pred_merged, left_index=True, right_index=True)\n",
    "    y_true_merged = pd.merge(X_df, y_true_merged, left_index=True, right_index=True)   \n",
    "    y_true_merged = y_true_merged.groupby('date')['stockPrice'].mean().reset_index()\n",
    "    print(y_true_merged)\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(y_pred_merged, y_pred_merged['stockPrice'], label = 'Predicted Price', color = 'red')\n",
    "    plt.plot(y_true_merged, y_true_merged['stockPrice'], label = 'True Price', color = 'blue')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Stock Price')\n",
    "    plt.title('Predicted vs True Stock Price')\n",
    "    plt.xlim(pd.Timestamp('2015-01-01'), pd.Timestamp('2020-01-01'))\n",
    "    plt.legend()\n",
    "    plt.xticks(rotation = 45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plotPredictions(true_price['aapl'], predictions['aapl'], final_tweets['aapl'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for aapl:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'mean_absolute_error' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 33\u001b[0m\n\u001b[0;32m     31\u001b[0m y_true \u001b[38;5;241m=\u001b[39m true_price[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maapl\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     32\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m predictions[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maapl\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m---> 33\u001b[0m \u001b[43mcalculate_and_plot_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[10], line 4\u001b[0m, in \u001b[0;36mcalculate_and_plot_metrics\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcalculate_and_plot_metrics\u001b[39m(y_true, y_pred):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;66;03m# Calculate metrics\u001b[39;00m\n\u001b[0;32m      3\u001b[0m     mse \u001b[38;5;241m=\u001b[39m mean_squared_error(y_true, y_pred)\n\u001b[1;32m----> 4\u001b[0m     mae \u001b[38;5;241m=\u001b[39m \u001b[43mmean_absolute_error\u001b[49m(y_true, y_pred)\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# Print metrics\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMSE: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmse\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'mean_absolute_error' is not defined"
     ]
    }
   ],
   "source": [
    "def calculate_and_plot_metrics(y_true, y_pred):\n",
    "    # Calculate metrics\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "\n",
    "    # Print metrics\n",
    "    print(f\"MSE: {mse}\")\n",
    "    print(f\"MAE: {mae}\")\n",
    "\n",
    "    # Plot metrics\n",
    "    plt.figure(figsize=(10, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.title(\"MSE\")\n",
    "    plt.bar(['MSE'], [mse])\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.title(\"MAE\")\n",
    "    plt.bar(['MAE'], [mae])\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Calculate and plot metrics for each stock\n",
    "'''for stock in stocks:\n",
    "    print(f\"Metrics for {stock}:\")\n",
    "    y_true = true_price[stock]\n",
    "    y_pred = predictions[stock]\n",
    "    calculate_and_plot_metrics(y_true, y_pred)'''\n",
    "\n",
    "print(f\"Metrics for aapl:\")\n",
    "y_true = true_price['aapl']\n",
    "y_pred = predictions['aapl']\n",
    "calculate_and_plot_metrics(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming y_true, X_test are defined, and model is trained\n",
    "\n",
    "# Predict scores\n",
    "y_scores = y_pred\n",
    "\n",
    "# Find the optimal threshold\n",
    "print(y_scores)\n",
    "fpr, tpr, thresholds = roc_curve(y_true.astype(int), y_scores)\n",
    "optimal_idx = np.argmax(tpr - fpr)\n",
    "optimal_threshold = thresholds[optimal_idx]\n",
    "\n",
    "# Convert the predicted values to binary based on the optimal threshold\n",
    "y_pred_binary_optimal = np.where(y_scores > optimal_threshold, 1, 0)\n",
    "\n",
    "# Calculate metrics using the binary values with the optimal threshold\n",
    "accuracy_opt = accuracy_score(y_true.astype(int), y_pred_binary_optimal)\n",
    "precision_opt = precision_score(y_true.astype(int), y_pred_binary_optimal)\n",
    "recall_opt = recall_score(y_true.astype(int), y_pred_binary_optimal)\n",
    "f1_opt = f1_score(y_true.astype(int), y_pred_binary_optimal)\n",
    "roc_auc_opt = roc_auc_score(y_true.astype(int), y_scores)\n",
    "\n",
    "# Print metrics\n",
    "print(f\"Optimal Threshold: {optimal_threshold}\")\n",
    "print(f\"Precision: {precision_opt}\")\n",
    "print(f\"Recall: {recall_opt}\")\n",
    "print(f\"F1 Score: {f1_opt}\")\n",
    "print(f\"ROC AUC: {roc_auc_opt}\")\n",
    "\n",
    "# Plot metrics\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'ROC AUC']\n",
    "values_opt = [accuracy_opt, precision_opt, recall_opt, f1_opt, roc_auc_opt]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(metrics, values_opt, color='skyblue')\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.title('Performance Metrics with Optimized Threshold')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TwitterSentiment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
