{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, Input\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import yfinance as yf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oneHotEncode(df, columns):\n",
    "    all_years = [2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024]\n",
    "    all_months = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
    "    all_days = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]\n",
    "    all_days_of_week = [1, 2, 3, 4, 5, 6, 7]\n",
    "\n",
    "    one_hot_encoded_df = pd.get_dummies(df, columns = columns)\n",
    "    one_hot_encoded_df = one_hot_encoded_df.reindex(columns=[f'year_{year}' for year in all_years], fill_value=0)\n",
    "    one_hot_encoded_df = one_hot_encoded_df.reindex(columns=[f'month_{month}' for month in all_months], fill_value=0)\n",
    "    one_hot_encoded_df = one_hot_encoded_df.reindex(columns=[f'day_{day}' for day in all_days], fill_value=0)\n",
    "    one_hot_encoded_df = one_hot_encoded_df.reindex(columns=[f'day_of_week_{day_of_week}' for day_of_week in all_days_of_week], fill_value=0)\n",
    "\n",
    "    print(one_hot_encoded_df.head())\n",
    "    print(df.head())\n",
    "\n",
    "    new_dataframe = pd.concat([df, one_hot_encoded_df], axis=1)\n",
    "\n",
    "    return new_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of all stocks\n",
    "stocks = ['aapl', 'amzn', 'googl', 'msft', 'tsla']\n",
    "\n",
    "# Get rid of annoying errors that don't matter\n",
    "pd.set_option('mode.chained_assignment', None)\n",
    "\n",
    "# Create the scaler\n",
    "sentimentScaler = MinMaxScaler()\n",
    "stockScaler = MinMaxScaler()\n",
    "\n",
    "# Create the datasets from the csvs\n",
    "# Could use a lot of work to reduce loc but it doesn't matter for marks\n",
    "def dataFrames(stock):\n",
    "\n",
    "    # Create dataframes for the tweets and prices \n",
    "    tweet_df = pd.read_csv('csv/' + stock + '.csv')\n",
    "    tweet_1_df = pd.read_csv('csv/' + stock + '_1.csv')\n",
    "    price_df = pd.read_csv('csv/' + stock + '_price.csv')\n",
    "\n",
    "    # Rename the date column so they match between dataframes\n",
    "    tweet_df.rename(columns = {'DATE': 'date'}, inplace = True)\n",
    "    tweet_1_df.rename(columns = {'Date': 'date'}, inplace = True)\n",
    "    price_df.rename(columns = {'Date' : 'date', 'Close/Last': 'stockPrice'}, inplace = True)\n",
    "\n",
    "    # Rename the score column so they match between dataframes\n",
    "    tweet_df.rename(columns = {'TEXTBLOB_POLARITY': 'sentimentScore'}, inplace = True)\n",
    "    tweet_1_df.rename(columns = {'score': 'sentimentScore'}, inplace = True)\n",
    "\n",
    "    # Remove the $ sign in stock price, and convert columns to float64 (may be redundant)\n",
    "    price_df['stockPrice'] = price_df['stockPrice'].str.replace('$', '')\n",
    "    tweet_df['sentimentScore'] = pd.to_numeric(tweet_df['sentimentScore'])\n",
    "    tweet_1_df['sentimentScore'] = pd.to_numeric(tweet_1_df['sentimentScore'])\n",
    "    price_df['stockPrice'] = pd.to_numeric(price_df['stockPrice'])\n",
    "\n",
    "    # Convert the dates to pd datetime\n",
    "    price_df['date'] = pd.to_datetime(price_df['date'])\n",
    "    tweet_df['date'] = pd.to_datetime(tweet_df['date'], dayfirst = True)\n",
    "    tweet_1_df['date'] = pd.to_datetime(tweet_1_df['date'])\n",
    "\n",
    "    # Reduce to only nessicary columns\n",
    "    tweet_df = tweet_df[['date', 'sentimentScore']]\n",
    "    tweet_1_df = tweet_1_df[['date', 'sentimentScore']]\n",
    "    price_df = price_df[['date', 'stockPrice']]\n",
    "\n",
    "    # Merge the dataframes based on the date column\n",
    "    merged_1_df = pd.merge(tweet_df, price_df, on = 'date')\n",
    "    merged_2_df = pd.merge(tweet_1_df, price_df, on = 'date')\n",
    "    merged_df = pd.merge(merged_1_df, merged_2_df, how = 'outer')\n",
    "\n",
    "    # Drop all duplicates\n",
    "    final_df = merged_df.drop_duplicates()\n",
    "\n",
    "    # Convert date to specific variables\n",
    "    final_df['year'] = final_df['date'].dt.year\n",
    "    final_df['month'] = final_df['date'].dt.month\n",
    "    final_df['day'] = final_df['date'].dt.day\n",
    "    final_df['day_of_week'] = final_df['date'].dt.dayofweek\n",
    "\n",
    "    # One-hot encoding, should be done for all for financial data\n",
    "    #final_df = oneHotEncode(final_df, ['year', 'month', 'day', 'day_of_week'])\n",
    "    #final_df = pd.get_dummies(final_df, columns = ['year', 'month', 'day', 'day_of_week'])\n",
    "\n",
    "    # Drop the original date column\n",
    "    final_df = final_df.drop(columns = ['date'])\n",
    "\n",
    "    # Save to csv for double checking\n",
    "    #final_df.to_csv('final_' + stock + '.csv', index = False)\n",
    "\n",
    "    # Scale the numerical values\n",
    "    final_df[['sentimentScore']] = sentimentScaler.fit_transform(final_df[['sentimentScore']])\n",
    "    final_df[['stockPrice']] = stockScaler.fit_transform(final_df[['stockPrice']])\n",
    "\n",
    "    # Split into X and y\n",
    "    X = final_df.drop(columns = ['stockPrice'])\n",
    "    y = final_df['stockPrice']\n",
    "\n",
    "    # Split the datasets into training and test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
    "\n",
    "    '''# Reshape the datasets\n",
    "    X_train_reshaped = reshape_data(X_train)\n",
    "    X_test_reshaped = reshape_data(X_test)'''\n",
    "\n",
    "    # NOTE: Sequence length (1) could be improved by making one of the commented functions above work.\n",
    "    X_train_reshaped = np.reshape(X_train, (-1, 1, X_train.shape[1]))\n",
    "    X_test_reshaped = np.reshape(X_test, (-1,  1, X_test.shape[1]))\n",
    "    y_train_array = y_train.values\n",
    "    y_test_array = y_test.values\n",
    "\n",
    "    # Return the training and test datasets\n",
    "    return X_train_reshaped, X_test_reshaped, y_train_array, y_test_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aapl X_train shape: (21542, 1, 5)\n",
      "aapl y_train shape: (21542,)\n",
      "amzn X_train shape: (19976, 1, 5)\n",
      "amzn y_train shape: (19976,)\n",
      "googl X_train shape: (16819, 1, 5)\n",
      "googl y_train shape: (16819,)\n",
      "msft X_train shape: (14943, 1, 5)\n",
      "msft y_train shape: (14943,)\n",
      "tsla X_train shape: (20452, 1, 5)\n",
      "tsla y_train shape: (20452,)\n"
     ]
    }
   ],
   "source": [
    "all_data = {}  # Dictionary to hold data for all stocks\n",
    "\n",
    "for stock in stocks:\n",
    "    # Run the 'dataFrames' function for each stock\n",
    "    data = dataFrames(stock)\n",
    "    \n",
    "    # Store the datasets in the dictionary\n",
    "    all_data[stock] = {\n",
    "        'X_train': data[0],\n",
    "        'X_test': data[1],\n",
    "        'y_train': data[2],\n",
    "        'y_test': data[3]\n",
    "    }\n",
    "\n",
    "    # Print the shapes of X_train and y_train for the current stock\n",
    "    print(f\"{stock} X_train shape: {data[0].shape}\")\n",
    "    print(f\"{stock} y_train shape: {data[2].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169/169 [==============================] - 1s 4ms/step - loss: 0.0489\n",
      "aapl Test Loss:  0.04890572279691696\n",
      "157/157 [==============================] - 1s 4ms/step - loss: 0.0973\n",
      "amzn Test Loss:  0.09730158746242523\n",
      "132/132 [==============================] - 1s 4ms/step - loss: 0.0657\n",
      "googl Test Loss:  0.06572912633419037\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 0.0738\n",
      "msft Test Loss:  0.0737779513001442\n",
      "160/160 [==============================] - 1s 3ms/step - loss: 0.0376\n",
      "tsla Test Loss:  0.03758905455470085\n"
     ]
    }
   ],
   "source": [
    "# Initialize dictionary to store the models for each stock\n",
    "models = {}\n",
    "\n",
    "for stock, datasets in all_data.items():\n",
    "    X_train = datasets['X_train']\n",
    "    y_train = datasets['y_train']\n",
    "    X_test = datasets['X_test']\n",
    "    y_test = datasets['y_test']\n",
    "    \n",
    "    # Define the model for current stock\n",
    "    model = Sequential([\n",
    "        Input(shape=X_train.shape[1:]),\n",
    "        LSTM(units=50, return_sequences=True, input_shape=(X_train.shape[1:], 1)),\n",
    "        Dropout(0.2), \n",
    "        LSTM(units=50, return_sequences=True),\n",
    "        Dropout(0.2),\n",
    "        LSTM(units=50),\n",
    "        Dropout(0.2),\n",
    "        Dense(units=1)\n",
    "    ])\n",
    "\n",
    "    # Convert data type of input arrays to float32\n",
    "    X_train = X_train.astype('float32')\n",
    "    y_train = y_train.astype('float32')\n",
    "    X_test = X_test.astype('float32')\n",
    "    y_test = y_test.astype('float32')\n",
    "\n",
    "    # Compile model using the Adam optimizer\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "\n",
    "    # Ensure that X_train and y_train have the same number of samples (if necessary)\n",
    "    X_train = X_train[:len(y_train)]\n",
    "\n",
    "    # Fit the model with verbose settings for debugging\n",
    "    model.fit(X_train, y_train, epochs=1, batch_size=32, verbose=0)\n",
    "\n",
    "    # Evaluate the model and print the loss\n",
    "    loss = model.evaluate(X_test, y_test)\n",
    "    print(f\"{stock} Test Loss: \", loss)\n",
    "\n",
    "    # Store the model in the dictionary\n",
    "    models[stock] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aapl Random Forest Test Loss:  4.1968745959621414e-10\n",
      "aapl Linear Regression Test Loss:  0.011016191169306345\n",
      "aapl Decision Tree Test Loss:  1.9007839717704197e-32\n",
      "amzn Random Forest Test Loss:  1.1315437327411813e-09\n",
      "amzn Linear Regression Test Loss:  0.010219437240305293\n",
      "amzn Decision Tree Test Loss:  3.912598012761351e-32\n",
      "googl Random Forest Test Loss:  5.083253809348119e-09\n",
      "googl Linear Regression Test Loss:  0.006389820932366614\n",
      "googl Decision Tree Test Loss:  3.846647740672896e-32\n",
      "msft Random Forest Test Loss:  1.955834244360713e-09\n",
      "msft Linear Regression Test Loss:  0.00498345298212116\n",
      "msft Decision Tree Test Loss:  7.520586315893814e-33\n",
      "tsla Random Forest Test Loss:  8.65813642465698e-09\n",
      "tsla Linear Regression Test Loss:  0.02937480932458805\n",
      "tsla Decision Tree Test Loss:  1.2126277193947334e-32\n"
     ]
    }
   ],
   "source": [
    "for stock, datasets in all_data.items():\n",
    "    X_train = datasets['X_train']\n",
    "    y_train = datasets['y_train']\n",
    "    X_test = datasets['X_test']\n",
    "    y_test = datasets['y_test']\n",
    "\n",
    "    # Flatten X data for compatibility with RandomForest, Linear Regression, and Decision Tree\n",
    "    num_samples, timesteps, num_features = X_train.shape\n",
    "    X_train_2d = X_train.reshape(num_samples, timesteps * num_features)\n",
    "    X_test_2d = X_test.reshape(X_test.shape[0], timesteps * num_features)\n",
    "\n",
    "    # Store processed data\n",
    "    all_data[stock] = (X_train_2d, X_test_2d, y_train, y_test)\n",
    "\n",
    "# Initialize dictionaries for models and their performances\n",
    "rf_models = {}\n",
    "rf_losses = {}\n",
    "lr_models = {}\n",
    "lr_losses = {}\n",
    "dt_models = {}\n",
    "dt_losses = {}\n",
    "\n",
    "for stock, (X_train_2d, X_test_2d, y_train, y_test) in all_data.items():\n",
    "    # RandomForestRegressor\n",
    "    rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    rf_model.fit(X_train_2d, y_train)\n",
    "    rf_predictions = rf_model.predict(X_test_2d)\n",
    "    rf_loss = mean_squared_error(y_test, rf_predictions)\n",
    "    rf_models[stock] = rf_model\n",
    "    rf_losses[stock] = rf_loss\n",
    "    print(f\"{stock} Random Forest Test Loss: \", rf_loss)\n",
    "\n",
    "    # Linear Regression\n",
    "    lr_model = LinearRegression()\n",
    "    lr_model.fit(X_train_2d, y_train)\n",
    "    lr_predictions = lr_model.predict(X_test_2d)\n",
    "    lr_loss = mean_squared_error(y_test, lr_predictions)\n",
    "    lr_models[stock] = lr_model\n",
    "    lr_losses[stock] = lr_loss\n",
    "    print(f\"{stock} Linear Regression Test Loss: \", lr_loss)\n",
    "    \n",
    "    # Decision Tree Regressor\n",
    "    dt_model = DecisionTreeRegressor(random_state=42)\n",
    "    dt_model.fit(X_train_2d, y_train)\n",
    "    dt_predictions = dt_model.predict(X_test_2d)\n",
    "    dt_loss = mean_squared_error(y_test, dt_predictions)\n",
    "    dt_models[stock] = dt_model\n",
    "    dt_losses[stock] = dt_loss\n",
    "    print(f\"{stock} Decision Tree Test Loss: \", dt_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aapl Gaussian Naive Bayes Test Accuracy:  0.9658373561084292\n",
      "amzn Gaussian Naive Bayes Test Accuracy:  0.9983983983983984\n",
      "googl Gaussian Naive Bayes Test Accuracy:  0.9938168846611177\n",
      "msft Gaussian Naive Bayes Test Accuracy:  0.9352248394004282\n",
      "tsla Gaussian Naive Bayes Test Accuracy:  0.9822022296107961\n"
     ]
    }
   ],
   "source": [
    "# Initialize dictionaries for GaussianNB models and their accuracies\n",
    "gaussian_nb_models = {}\n",
    "gaussian_nb_accuracies = {}\n",
    "\n",
    "# Label encoder to convert continuous labels to discrete integers\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "for stock, (X_train_2d, X_test_2d, y_train, y_test) in all_data.items():\n",
    "    # Convert continuous labels to discrete integers\n",
    "    y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "    y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "    # Define and initialize the Gaussian Naive Bayes model\n",
    "    bayes_model = GaussianNB()\n",
    "\n",
    "    # Fit the model with the reshaped 2D training data and encoded labels\n",
    "    bayes_model.fit(X_train_2d, y_train_encoded)\n",
    "\n",
    "    # Make predictions on the reshaped 2D test data\n",
    "    bayes_predictions = bayes_model.predict(X_test_2d)\n",
    "\n",
    "    # Calculate and print the accuracy for the test set predictions\n",
    "    accuracy = accuracy_score(y_test_encoded, bayes_predictions)\n",
    "    print(f\"{stock} Gaussian Naive Bayes Test Accuracy: \", accuracy)\n",
    "\n",
    "    # Store the model and its accuracy\n",
    "    gaussian_nb_models[stock] = bayes_model\n",
    "    gaussian_nb_accuracies[stock] = accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 1, 5)\n",
      "(5386, 1, 5)\n",
      "169/169 [==============================] - 1s 5ms/step\n",
      "[[18.379349]\n",
      " [18.379349]\n",
      " [18.379349]\n",
      " ...\n",
      " [18.379349]\n",
      " [18.379349]\n",
      " [18.379349]]\n"
     ]
    }
   ],
   "source": [
    "predictions = {}\n",
    "true_price = {}\n",
    "symbols = ['AAPL', 'AMZN', 'GOOGL', 'MSFT', 'TSLA']\n",
    "\n",
    "def predictData(symbol, stock = 'aapl'):\n",
    "\n",
    "    # Get the model for the stock\n",
    "    current_price_data = [[168.45]]\n",
    "    current_price_data = pd.DataFrame(current_price_data, columns=['stockPrice'])\n",
    "    current_price_data['stockPrice'] = pd.to_numeric(current_price_data['stockPrice'])\n",
    "    current_price_data['stockPrice'] = stockScaler.transform(current_price_data[['stockPrice']])\n",
    "    \n",
    "    # Get the true price\n",
    "    y_true = current_price_data.values\n",
    "\n",
    "    # Get the current tweets\n",
    "    current_tweets = pd.read_csv('csv/output.csv')\n",
    "    current_tweets.rename(columns = {'Date': 'date'}, inplace = True)\n",
    "    current_tweets.rename(columns = {'Sentiment': 'sentimentScore'}, inplace = True)\n",
    "    \n",
    "    # Remove the $ sign in stock price, and convert columns to float64 (may be redundant)\n",
    "    sentiment_mapping = {'Negative': -1, 'Neutral': 0, 'Positive': 1}\n",
    "    current_tweets['sentimentScore'] = current_tweets['sentimentScore'].map(sentiment_mapping)\n",
    "    current_tweets = current_tweets[['date', 'sentimentScore']]\n",
    "    current_tweets['sentimentScore'] = pd.to_numeric(current_tweets['sentimentScore'])\n",
    "    current_tweets['sentimentScore'] = sentimentScaler.transform(current_tweets[['sentimentScore']])\n",
    "\n",
    "    # Convert the dates to pd datetime\n",
    "    current_tweets['date'] = pd.to_datetime(current_tweets['date'])\n",
    "    current_tweets['year'] = current_tweets['date'].dt.year\n",
    "    current_tweets['month'] = current_tweets['date'].dt.month\n",
    "    current_tweets['day'] = current_tweets['date'].dt.day\n",
    "    current_tweets['day_of_week'] = current_tweets['date'].dt.dayofweek\n",
    "\n",
    "    # Final changes\n",
    "    X = current_tweets.drop(columns = ['date'])\n",
    "    X = np.reshape(X, (-1, 1, X.shape[1]))\n",
    "    X = X.astype('float32')\n",
    "    print(X.shape)\n",
    "\n",
    "    # Use old data\n",
    "    '''X = all_data['aapl'][1]\n",
    "    X = np.reshape(X, (-1, 1, X.shape[1]))\n",
    "    print(X.shape)'''\n",
    "\n",
    "    predictions[stock] = stockScaler.inverse_transform(model.predict(X))\n",
    "    true_price[stock] = y_true\n",
    "\n",
    "# Predict for each stock\n",
    "'''for stock in stocks:\n",
    "    print(f\"Predictions for {stock}:\")\n",
    "    predictData(stock)'''\n",
    "\n",
    "predictData('AAPL')\n",
    "\n",
    "print(predictions['aapl'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TwitterSentiment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
